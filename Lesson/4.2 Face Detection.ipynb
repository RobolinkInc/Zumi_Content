{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<img src=\"../Data/images/ZumiHeader.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# Face Detection\n",
    "\n",
    "<font size =3> How does face detection work? Start by thinking about how humans detect faces. Do you think a computer does the same thing? </font>\n",
    "\n",
    "\n",
    "## Haar cascade\n",
    "\n",
    "<font size =3> Before looking at a face, let's look at an example that has fewer features. You have no problem identifying the sign below as a stop sign, but how do you know it's a stop sign? What difficulties will a computer have in trying to do the same task?\n",
    "</font>\n",
    "<br><br>\n",
    "\n",
    "<img src=\"../Data/images/blank_stop_sign.png\" width=200>\n",
    "\n",
    "<font size =3> The easiest way to identify that this is a stop sign is by identifying particular **features**, such as color and shape. You can train a computer to recognize a stop sign by collefcting multiple pictures of stop signs, called positive images, as well as images that do not have stop signs, called negative images. The model that is produced is known as a **Haar cascade**. This process is similar to how humans learn. We are exposed to signs that are stop signs and to signs that are not.</font>\n",
    "\n",
    "### Haar features\n",
    "<font size =3> A Haar cascade is named after a mathematical function, but it was invented by Paul Viola and Michael Jones in 2001 (Source). They based their algorithm on the idea that certain features, called **Haar features**, can represent objects. <br>\n",
    "Think back to the lesson on color classification. Remember that an image is a set of pixels, and each pixel in the color image is normally represented by a set of three numbers. Now convert that stop sign image to grayscale. \n",
    "\n",
    "Source: http://www.willberger.org/cascade-haar-explained/ <br> <br>\n",
    "\n",
    "\n",
    "<img src=\"../Data/images/convert_stop_sign.png\" width=500>\n",
    "\n",
    "In a grayscale image, you only need one number to represent each pixel instead of three. The lowest value is 0, representing black, and the highest value is 255, representing white. Any value in between is a shade of grey.\n",
    "\n",
    "<img src=\"../Data/images/grayscale_values.png\" width=500>\n",
    "\n",
    "With this in mind, you can start to understand Haar features. In a stop sign, there will be a certain pattern of lighter pixels representing the letters in \"STOP\" and slightly darker pixels surrounding them. Now apply this concept to a human face. Faces have multiple features, incuding eyes, noses, and mouths. If you feed the computer multiple grayscale images of faces, it will start to find a pattern in the average pixel values. Down the bridge of the nose, it is more likely to find a column of lighter pixels surrounded by darker pixels on the sides. The computer might also find that the area with our eyes and eyebrows generally has darker pixels than our cheeks. \n",
    "\n",
    "<img src=\"../Data/images/haar_cascade.png\" width=500>\n",
    "\n",
    "Even though the computer isn’t aware of what it’s looking at, it can see pixel value patterns in certain arrangements (for example, your nose wouldn’t be above your eyes) to classify the image as a human face. These are the Haar features that the program will be looking for when presented with a new image it has never seen before.</font>\n",
    "\n",
    "## Code\n",
    "<font size =3> Training a Haar Cascade is time-consuming. To make a really good classifier, you would need thousands of positive and negative images and some computing power. For detecting faces, the cascade file has already been provided for you! It’s a file with thousands of lines of code that contain all of the information for classifying whatever it was trained to detect. </font>\n",
    "\n",
    "### Import libraries\n",
    "<font size =3>First, you need to import all of the necessary libraries. You will need the camera library for taking pictures, the vision library for detecting faces, and the screen library for drawing on the screen. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Zumi \n",
      "Pi Zero I2C is available\n",
      "Verified Pi Zero is the same\n",
      "Gyroscope previously calibrated\n",
      "Zumi board detected\n",
      "Compass detected\n",
      "OLED Screen detected\n",
      "Gyroscope & Accelerometer detected\n"
     ]
    }
   ],
   "source": [
    "from zumi.util.camera import Camera \n",
    "from zumi.zumi import Zumi\n",
    "from zumi.util.screen import Screen\n",
    "from zumi.util.vision import Vision\n",
    "import time\n",
    "\n",
    "\n",
    "zumi = Zumi()\n",
    "camera = Camera()\n",
    "screen = Screen()\n",
    "vision = Vision()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Grayscale and find_face()\n",
    "\n",
    "<font size=3> For a Haar cascade to calculate pixel differences, it needs a grayscale image as input. In the vision library, there is a function that you can use that accepts an image parameter and returns the same image in grayscale. For example, if you take an image and save it in <font face=\"Courier\">image</font>, you can convert the color image to grayscale. See Lesson 4.1 if you need a refresher!<br>\n",
    "\n",
    "<font face=\"Courier\"> image = vision.convert_to_gray(image) </font> <br><br>\n",
    "   \n",
    "**Note:** The computer vision library will always convert it to grayscale in the background for you, but it's a good habit to convert the image yourself as a reminder of how Haar Cascades work. <br><br>\n",
    "    \n",
    "Now, you can use this grayscale version of the image in the function <font face=\"Courier\">vision.find_face()</font>. This function will look for the Haar features that represent a face and draw a box around any faces the algorithm finds. Here is how you call the function:\n",
    "\n",
    "<font face=\"Courier\"> vision.find_face(image)</font><br>\n",
    "    \n",
    "Finally, don't forget to show the image on the screen with <font face=\"Courier\"> camera.show_image()</font>. If you need a review on any of the camera functions, refer to lesson 4.1. </font> \n",
    "<br><br>\n",
    "\n",
    "### Pseudocode\n",
    "<font size=3> We know that's a lot to remember, so we left you some pseudocode to follow! <br> \n",
    "    \n",
    "<font face=\"Courier\"> \n",
    "Start camera <br>\n",
    "take an image<br>\n",
    "close the camera<br>\n",
    "convert the image to gray<br>\n",
    "use find_face() to find a face<br>\n",
    "show the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here to detect a face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying parameters\n",
    "<font size=3> Try taking pictures with your face close up and very far away. How close does your face need to be for the camera to classify it as a face? How far away? The Haar cascade actually has some default values that determine how good your algorithm will be at detecting faces. Depending on your use case, these can be changed. </font>\n",
    "    \n",
    "### minSize\n",
    "\n",
    "<font size=3> Setting a minimum size will only return faces that it finds bigger than (x,y) area. </font>\n",
    "\n",
    "### maxSize\n",
    "\n",
    "<font size=3> Setting a maximum size will only return faces that it finds smaller than (x,y) area. </font>\n",
    "\n",
    "### minNeighbors\n",
    "\n",
    "<font size=3> Each feature is a rectangle of pixels. Neighbors are any features that have similarities. If you increase this number, it will detect fewer faces because the conditions for meeting the requirements of a face require more similar features. </font>\n",
    "\n",
    "### scaleFactor\n",
    "<font size=3> In the Color and Signs training lessons, we mentioned images are often reduced in size to make training easier. This is a computationally heavy process, so the default is reducing the image by 5% at each scale.\n",
    "    \n",
    "Here is the complete function with all the default parameters: <br> <br>  \n",
    "    \n",
    "<font face=\"Courier\">vision.find_face(image, scale_factor = 1.05, min_neighbors=8, min_size=(40,40))</font><br><br>\n",
    "\n",
    "Start changing the numbers slowly and see how this affects the detection algorithm. Find some objects that might look like a face. If you change the parameters, can the computer tell the different between a real face and something else?\n",
    "    \n",
    "Bonus: Write the code in a for loop to show detection as a video!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Code here that modifies these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Decisions\n",
    "<font size = 3> In this activity, Zumi will keep her eyes on you! You will write code so that Zumi eyes follow your face when it's the middle, left, or right side of the frame. If Zumi can't see you, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
