{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<img src=\"../Data/images/ZumiHeader.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# Face Detection\n",
    "\n",
    "<font size =3> You have probably already tried the Smile and Drive or Who am I? demos and seen that Zumi can detect faces. How does this work? Start by thinking about how humans detect faces. Does a computer do the same thing? </font>\n",
    "\n",
    "\n",
    "## Haar Cascade\n",
    "\n",
    "<font size =3> Before looking at a face, let's discuss an example that's has fewer features. You have no problem identifying the sign below as a stop sign, but how do you know? What difficulties will a computer have in trying to do the same task?\n",
    "</font>\n",
    "\n",
    "<img src=\"../Data/images/blank_stop_sign.png\" width=200>\n",
    "\n",
    "<font size =3> The easiest way to identify that this is a stop sign is by identifying particular **features**, such as color and shape. You can train a computer to recognize a stop sign by collefcting multiple pictures of stop signs, called positive images, as well as images that do not have stop signs, called negative images. The model that is produced is known as a **Haar Cascade**. This process is similar to how humans learn. We are exposed to signs that are stop signs and exposed to signs that are not.</font>\n",
    "\n",
    "### Haar Features\n",
    "<font size =3> Where do Haar Cascades come from? A Haar Cascade is named after a mathematical function, but it was invented by Paul Viola and Michael Jones in 2001 (Source). They based their algorithm on the idea that certain features, **Haar Features**, can represent objects Think back to the lesson on color classification. Remember that an image is a set of pixels and that each pixel in the color image is normally represented by a set of three numbers. Now imagine that we convert that stop sign photo to grayscale. \n",
    "\n",
    "Source: http://www.willberger.org/cascade-haar-explained/ <br> <br>\n",
    "\n",
    "\n",
    "<img src=\"../Data/images/convert_stop_sign.png\" width=500>\n",
    "\n",
    "How many numbers do you need to represent each pixel? In a grayscale image, you only need one number per pixel instead of three. The lowest value is 0 representing black and the highest value is 255 representing white. Any value in between is a shade of grey.\n",
    "\n",
    "\n",
    "<img src=\"../Data/images/grayscale_values.png\" width=500>\n",
    "\n",
    "With this in mind, you can understand Haar features. In a stop sign, there will be a certain pattern of lighter pixels representing the letters \"STOP\" and slightly darker pixels surrounding it. Now apply this concept to a face. Faces have multiple features. Eyes, noses, mouths, etc. If we feed the computer multiple grayscale images of faces, it will start to find a pattern in the average pixel values. Down the bridge of the nose, it is more likely to find a column of lighter pixels surrounded by darker pixels on the sides. The computer might also find that the area with our eyes and eyebrow generally has darker pixels than our cheeks. \n",
    "\n",
    "\n",
    "<img src=\"../Data/images/haar_cascade.png\" width=500>\n",
    "\n",
    "Even though the computer isn’t aware of what it’s looking at, it can see pixel value patterns in certain arrangements (your nose wouldn’t be above your eyes) to classify the image as a human face. These features are the Haar features that the program will be looking for when presented with a new image it has never seen before.</font>\n",
    "\n",
    "## Code\n",
    "<font size =3> Training a Haar Cascade is time-consuming. To make a really good classifier you would need thousands of positive and negative images and some computing power. For detecting faces, the cascade file has already been provided for you. It’s a file with thousands of lines of code that contains all of the information for classifying whatever it was trained to detect. </font>\n",
    "\n",
    "### Import libraries\n",
    "<font size =3>First, you need to import all of the necessay libraries. You will need the camera library for taking pictures, vision library for detecting faces, and the screen library for drawing on the screen! </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/home/pi/zumi-python-library\")\n",
    "\n",
    "from zumi.util.camera import Camera \n",
    "from zumi.zumi import Zumi\n",
    "from zumi.util.screen import Screen\n",
    "from zumi.util.vision import Vision\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "zumi = Zumi()\n",
    "camera = Camera()\n",
    "screen = Screen()\n",
    "vision = Vision()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Grayscale and find_face()\n",
    "\n",
    "<font size=3> For a Haar Cascade to calculate pixel differences, it needs a grayscale image as input. In the vision library, there is a function that you can use that accepts an image parameter and returns the same image in grayscale. For example, if you take an image and save it in<font face=\"Courier\">image</font>, you can convert the color image to grayscale like this:<br><Br>\n",
    "\n",
    "<font face=\"Courier\"> image = vision.convert_to_gray(image) </font> <br><br>\n",
    "    \n",
    "Now, you can pass in this grayscale version of the image to the function <font face=\"Courier\">vision.find_face()</font>. This function will look for the Haar Features that represent a face and draw a box around an faces the algorithm finds. Here is how you call the function:\n",
    "\n",
    "<font face=\"Courier\"> vision.find_face(image)</font><br><br>\n",
    "    \n",
    "Finally, don't forget to show the image on the screen with <font face=\"Courier\"> vision.show_image()</font>. If you need a review on any of the camera functions, refer to lesson 4.1. </font> \n",
    "\n",
    "### Pseudocode\n",
    "<font size=3> We know that's a lot to remember, so left you some pseudocode to follow! <br> \n",
    "    \n",
    "<font face=\"Courier\"> \n",
    "Start camera <br>\n",
    "take an image<br>\n",
    "close the camera<br>\n",
    "convert the image to gray<br>\n",
    "try and find a face in the image<br>\n",
    "show the image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here to detect a face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying parameters\n",
    "<font size=3> Try taking pictures with your face close up and very far away. How close does your face need to be to the camera to classify it as a face? How far away? The Haar Cascade actually has some default values that determine how good your algorithm will be at detecting faces. Depending on your use case, these can be changed. </font>\n",
    "    \n",
    "### minSize\n",
    "\n",
    "<font size=3> Setting a minimum size will only return faces that it finds bigger than (x,y) area. </font>\n",
    "\n",
    "### maxSize\n",
    "\n",
    "<font size=3> Setting a maximum size will only return faces that it finds smaller than (x,y) area. </font>\n",
    "\n",
    "### minNeighbors\n",
    "\n",
    "<font size=3> Each feature is a rectangle of pixels. Neighbors are any features that have similarities. Increase this number and it will detect fewer faces because the conditions for meeting the requirements of a face require more similar features. </font>\n",
    "\n",
    "### scaleFactor\n",
    "<font size=3> Remember from the Color and Signs training lessons that we mentioned images are often reduced in size to make training easier. This is a computationally heavy process so the default is reducing the image by 5% at each scale.\n",
    "    \n",
    "Here is the complete function with all the default parameters: <br> <br> \n",
    "    \n",
    "<font face=\"Courier\">vision.find_face(image, scale_factor = 1.05, min_neighbors=8, min_size=(40,40))</font>\n",
    "\n",
    "Start changing the numbers slowly and see how this affects the detection algorithm. Find some objects that might look like a face. If you change the parameters, can the computer tell the different between a real face and something else?\n",
    "    \n",
    "Bonus: Write the code in a for loop\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Code here that modifies these parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
