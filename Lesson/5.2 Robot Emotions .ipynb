{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<img src=\"../Data/images/ZumiHeader.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# Robot Emotions\n",
    "\n",
    "<font size =3> Zumi has a personality! In this lesson, you will learn how Zumi detects human emotions as well as how to program Zumiâ€™s personality. You will also learn about sound, how itâ€™s measured, and how it corresponds with emotion.  Finally, you will train your Zumi to recognize and react to her favorite color. </font> \n",
    "\n",
    "\n",
    "\n",
    "## How do we detect emotion?\n",
    "<font size =3> Take a look at the images below and see if you can identify each of the emotions. \n",
    "\n",
    "<img src=\"../Data/images/emotions.png\" width=700> <br>\n",
    "\n",
    "How did you determine which emotion was which? There are many features that can be indicators, like the eyes, mouth, eyebrows, and maybe gestures. How do we translate human emotions to a robot?\n",
    "\n",
    "If you have seen the movie *Cars*, you may know that each of the cars has a personality. How was each car able to express emotions? Was it through movements? Sounds? Eyes? </font> \n",
    "\n",
    "\n",
    "### Import libraries\n",
    "<font size =3> To use personality functions, we need to import the Zumi, screen, and personality libraries. </font> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Zumi \n",
      "Pi Zero I2C is available\n",
      "Verified Pi Zero is the same\n",
      "Gyroscope previously calibrated\n",
      "Zumi board detected\n",
      "OLED Screen detected\n",
      "Gyroscope & Accelerometer detected\n"
     ]
    }
   ],
   "source": [
    "from zumi.zumi import Zumi\n",
    "from zumi.util.camera import Camera\n",
    "from zumi.util.screen import Screen\n",
    "from zumi.util.vision import Vision\n",
    "from zumi.personality import Personality\n",
    "from zumi.util.color_classifier import ColorClassifier\n",
    "import time\n",
    "\n",
    "zumi = Zumi()\n",
    "screen = Screen()\n",
    "personality = Personality(zumi, screen)\n",
    "camera = Camera()\n",
    "vision = Vision()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "###  Calling personality functions\n",
    "<font size =3> Here are some functions you can call:\n",
    "\n",
    "* happy()\n",
    "     \n",
    "* celebrate()\n",
    "       \n",
    "* angry()\n",
    "       \n",
    "* look_around()\n",
    "\n",
    "* look_around_open()\n",
    "       \n",
    "* disoriented_left()\n",
    "       \n",
    "* disoriented_right()\n",
    "\n",
    "* awake()\n",
    "\n",
    "For example, \n",
    "<font face=\"Courier\">personality.happy()</font> will make Zumi wiggle and make a sound!\n",
    "            \n",
    "In the cell below, try testing out some of the personality functions to see what they do. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Test Personality code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Sounds\n",
    "\n",
    "<font size =3>Zumi can play sounds to match her emotions! Sound can be measured in frequency and amplitude. \n",
    "\n",
    "*   Frequency is the number of pulses or vibrations per second, and is measured in hertz. The higher the frequency, the higher the pitch of the sound is. \n",
    "*   Amplitude is how loud or strong the sound is and is measured in decibels. The higher the amplitude, the louder the sound is. \n",
    "\n",
    "Video: [Sound: Wavelength, Frequency, and Amplitude](https://www.youtube.com/watch?v=TsQL-sXZOLc)\n",
    "\n",
    "What does each emotion sounds like? Is happy a low or high frequency? Is angry a low or high amplitude? How does this apply to Zumi?\n",
    "\n",
    "You can use <font face=\"Courier\">play_note()</font> to play various notes. The first parameter is the note you want to play (anywhere from C2 to B6). The second parameter is optional and denotes the amount of time you want the note to play in milliseconds. The default value is set to 500ms, but you can change that by adding a second parameter like this: <br><br>\n",
    "<font face=\"Courier\">play_note(Note.GS3, 400)</font>. <br><br>\n",
    "This plays the note G Sharp below middle C for 400 milliseconds. Try the code below to hear a scale and then compose your own music!</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from zumi.protocol import Note \n",
    "zumi.play_note(Note.C4)\n",
    "zumi.play_note(Note.D4)\n",
    "zumi.play_note(Note.E4)\n",
    "zumi.play_note(Note.F4)\n",
    "zumi.play_note(Note.G4)\n",
    "zumi.play_note(Note.A4)\n",
    "zumi.play_note(Note.B4)\n",
    "zumi.play_note(Note.C5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<font size =3> Code your own sounds for happy, sad, angry, or excited. Try out different melodies until you find your favorites. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Make your melodies here ðŸŽµ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Screen\n",
    "\n",
    "<font size =3> Zumi personality also uses the **OLED** (organic LED) screen to display emotions.\n",
    "There are many different \"eyes\" Zumi has:\n",
    "\n",
    "* <font face=\"Courier\"> close_eyes()</font>\n",
    "* <font face=\"Courier\"> sleepy_eyes()</font>\n",
    "* <font face=\"Courier\"> sleepy_left()</font>\n",
    "* <font face=\"Courier\"> sleepy_right()</font>\n",
    "* <font face=\"Courier\"> blink()</font>\n",
    "* <font face=\"Courier\"> look_around_open()</font>\n",
    "* <font face=\"Courier\"> sleeping()</font>\n",
    "* <font face=\"Courier\"> look_around()</font>     \n",
    "* <font face=\"Courier\"> glimmer()</font>\n",
    "* <font face=\"Courier\"> sad()</font>\n",
    "* <font face=\"Courier\"> happy()</font>\n",
    "* <font face=\"Courier\"> hello()</font>\n",
    "* <font face=\"Courier\"> angry()</font>\n",
    "\n",
    "To use the screen, call the screen class with a function of your choice. Try this: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "screen.sad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "### Draw Text\n",
    "\n",
    "<font size =3> Aside from drawing Zumi eyes, you can also have Zumi write messages on the screen! Use the <font face=\"Courier\">draw_text()</font> function to write a message like this: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "screen.draw_text(\"hello!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =3> If you want to automatically center the text on the screen, call this function instead: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen.draw_text_center(\"hello!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<font size =3> If you want to write text with numbers, you need to make sure everything is of the <font face=\"Courier\">String</font> data type. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "number = 10\n",
    "screen.draw_text(\"ten \" + str(number)) # the str() functions turns the number into a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<font size =3> You can even make Zumi display the time for you! </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,50):\n",
    "    screen.draw_text_center(time.ctime())\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You Smile, I Smile\n",
    "\n",
    "<font size=3> Now that you have learned to use sounds and screen to give Zumi a personality, have her react to a smile! In a previous lesson, you probably learned about using <font face=\"Courier\">vision.find_face()</font> to search for faces. What do you think the features are for a smile? Are the darker pixels of our mouths angled up or down? In this lesson, you will call a function to check if the face Zumi is detecting is smiling. If she sees a smile, she will be happy. If not, give Zumi some sad eyes.</font>\n",
    "    \n",
    "## vision.find_smile()\n",
    "<font size=3> The function you will call for this activity is <font face=\"Courier\">vision.find_smile()</font>. This function will return the pixel coordinates if Zumi finds a smile, or  <font face=\"Courier\">None</font> if she doesn't. For example, if you take a picture and save it in an image, you can check if Zumi found a smile or not: <br> <br>\n",
    "    \n",
    "<font face=\"Courier\">\n",
    "    \n",
    "smile = vision.find_smile(image)<br>\n",
    "if smile is not None: <br>\n",
    "<span style=\"margin-left: 40px;\"># Zumi sees a smile :)</span> <br>\n",
    "else: <br>\n",
    "<span style=\"margin-left: 40px;\"># Zumi does not see a smile :(</span> <br>\n",
    "    </font> </font>\n",
    "    \n",
    "## Code It!\n",
    "<font size=3> Now write some code that will take a picture with Zumi's camera and look for a smile. **Hint:** Make sure to change the parameters if Zumi can't find a smile. Give Zumi some personality and match her emotions with yours! Fill in the code template below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.start_camera()\n",
    "try:\n",
    "    for i in range(1000):\n",
    "        # Finish the code!\n",
    "\n",
    "        \n",
    "finally:\n",
    "    camera.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region of Interest\n",
    "<font size=3> If you heard someone say, \"Look at that dog!\", chances are you won't be looking up at the sky. That's because you know how to narrow your focus to where a dog is most likely to be found (walking on the ground). Computers can do the same thing to narrow their search. Since using a model to find faces and smiles takes up precious computation time, you will write a program that will only search for smiles where Zumi has already found a face.</font>\n",
    "\n",
    "    \n",
    "### Review Face Detection\n",
    "<font size=3> Rememeber from Lesson 4.2 that if Zumi found a face, the variable will return the coordinates of the face or <font face=\"Courier\">None</font> if no face was found. run the following code and make sure your face is in front of the Zumi camera. Can you guess what the numbers mean?\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.start_camera()\n",
    "\n",
    "try:\n",
    "    for i in range(50):\n",
    "        img = camera.capture()\n",
    "        gray = vision.convert_to_gray(img)\n",
    "        face = vision.find_face(gray)\n",
    "        print(face)\n",
    "finally:\n",
    "    camera.close()\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3> You have seen the console print out \"None\" or a list of four numbers which represent the coordinates of the face in this format: [x,y,width,height]. Here is how to save those values in their own variables. We shortened width and height to w and h to make it easier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.start_camera()\n",
    "\n",
    "try:\n",
    "    for i in range(50):\n",
    "        img = camera.capture()\n",
    "        gray = vision.convert_to_gray(img)\n",
    "        face = vision.find_face(gray)\n",
    "        if face is None:\n",
    "            print(\"no face\")\n",
    "        else:\n",
    "            x,y,w,h = face # This is possible because face has 4 values!\n",
    "            print(x,y,w,h)\n",
    "finally:\n",
    "    camera.close()\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The x and y are the coordinates of the top left corner of the bounding box that gets drawn around your face. The variables w and h are the width and height of the rectangle. <br></font>\n",
    "    \n",
    "[ Insert Image here ]\n",
    "    \n",
    "### Cropping an Image\n",
    "<font size=3> To select a certain area from an image, you need to specify the rows (y-values) and columns (x-values) of pixels you want in the cropped image. It will look something like this:<br>\n",
    "\n",
    "<font face=\"Courier\"> cropped = original_image[y1:y2,x1:x2] </font><br><br>\n",
    "In other words, the cropped image will be the rows from y1 to y2 and the columns from x1 to x2 of the original image.<br>\n",
    "    \n",
    "If x and y are the coordinates of the top left corner, then we know those are x1 and y1. But what are the coordinates of x2 and y2? **Hint:** Use the width and height to figure out the coordinates of the bottom right corner!\n",
    "    \n",
    "[Insert image here]\n",
    "    \n",
    "Now, you can make a cropped image cropped = original_image[y:y+h,x:x+w] and call find_smile() on the cropped image instead.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.start_camera()\n",
    "\n",
    "try:\n",
    "    for i in range(50):\n",
    "        img = camera.capture()\n",
    "        gray = vision.convert_to_gray(img)\n",
    "        face = vision.find_face(gray)\n",
    "        if face is None:\n",
    "            print(\"no face\")\n",
    "        else:\n",
    "            x,y,w,h = face # This is possible because face has 4 values!\n",
    "            print(x,y,w,h)\n",
    "            \n",
    "            # make a variable to hold the cropped image where the face is present\n",
    "            # find a smile in the cropped image\n",
    "finally:\n",
    "    camera.close()\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<hr>\n",
    "\n",
    "# Extension Activities <br> \n",
    "\n",
    "<img src=\"../Data/images/physics_extension.jpg\" width=70 align=\"left\">\n",
    "\n",
    "###  Frequency <br> \n",
    "<font size =3>Use tuners to identify the frequency, wavelength, and amplitude of different Zumi sounds. Which emotion has the highest or lowest frequency, wavelength, and amplitude? </font><br><br>\n",
    "\n",
    "\n",
    "### Add personality to color classifier lesson!\n",
    "<font size =3> <span style=\"color:red\"> **Note!** </span> This activity requires the color Training Wizard found on the \"Explore\" page.<br><br>\n",
    "Go back to the Color Training Wizard and train Zumi on a variety of colors. Load the model below and set a happy reaction to her favorite color and sad or angry reactions to other colors. Have a partner show Zumi various colors and guess what Zumi's favorite color is! </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = Camera()\n",
    "knn = ColorClassifier()\n",
    "train = knn.load_model(\"PROJECT NAME HERE\")\n",
    "knn.fit(\"hsv\")\n",
    "\n",
    "camera.start_camera()\n",
    "\n",
    "while True:\n",
    "        user_input = input(\"Press 'enter' to predict or 'q to quit: \")\n",
    "\n",
    "        if user_input == \"q\":\n",
    "            break\n",
    "            \n",
    "        image = camera.capture()\n",
    "        predict = knn.predict(image)\n",
    "        screen.draw_text_center(predict)\n",
    "        \n",
    "        # Add your if statements here!\n",
    "\n",
    "camera.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
