{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Data/images/ZumiHeader.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Sign Detection\n",
    "\n",
    "<font size =3>In this lesson, you will learn what a Haar Cascade is, how they work, and why they are important for object detection. In particular, Zumi will be able to detect a stop sign!\n",
    "\n",
    "You have no problem identifying the sign below as a stop sign, but how did you learn this? What difficulties will a computer have in trying to do the same task?</font>\n",
    "\n",
    "## Identifying objects\n",
    "\n",
    "<font size =3>You have no problem identifying the sign below as a stop sign, but how did you learn this? What difficulties will a computer have in trying to do the same task?</font>\n",
    "\n",
    "<img src=\"../Data/images/stop_sign.png\" width=200>\n",
    "\n",
    "<font size =3>The easiest way to identify that this is a stop sign is by the big letters that read “STOP”. However, a computer would have to learn to read and understand English before detecting the stop sign. Instead, you teach the computer by inputting multiple pictures of stop signs, called positive images, as well as images that **do not** have stop signs, called negative images. This is similar to how humans learn. We are exposed to signs that are stop signs and exposed to signs that are not. Which sign is this?</font>\n",
    "\n",
    "<img src=\"../Data/images/stop_sign_blank.png\" width=200>\n",
    "\n",
    "<font size =3>Easy, right? Even if we took away the word “STOP”, you would be able to pick out certain features that make it a stop sign: a red octagon. Computers are trained in a similar way to look for features.</font>\n",
    "\n",
    "## Feature Selection\n",
    "\n",
    "<font size =3>The Haar Cascade algorithm was developed based on a set of features called Haar-like features. Think back to the lesson on color classification. Remember that an image is a set of pixels and that each pixel in the color image is normally represented by a set of three numbers. Now imagine that we convert that photo to grayscale. \n",
    "</font>\n",
    "\n",
    "<img src=\"../Data/images/bw_stop_sign.png\" width=200>\n",
    "\n",
    "<font size =3>How many numbers do you need to represent each pixel? In a grayscale image, you only need one number per pixel instead of three. The lowest value is 0 representing black and the highest value is 255 representing white. Any value in between is a shade of grey. If you zoom in close enough to the image, you will be able to see individual pixels. In the image below, you are looking at the top of the \"S\" in \"STOP\".</font>\n",
    "\n",
    "<img src=\"../Data/images/stop_sign_zoomed.png\">\n",
    "\n",
    "<font size =3> Once the computer has seen positive and negative images, it can learn which images have stop signs and which do not. Even though the computer isn’t aware of what it’s looking at, it can see pixel value patterns in certain arrangements where pixels are lighter or darker. </font>\n",
    "\n",
    "## Haar Cascade File\n",
    "\n",
    "<font size=3> When a model is trained, all of the features are saved in the Haar cascade file. These files are .XML (extensible markup language) that can be used by the computer to detect a stop sign. In them are thousands of lines of code that contains all of the information for classifying whatever it was trained to detect. There are .XML cascade files available online for detecting faces, cars, street signs, smiles, and more. <br>\n",
    "    \n",
    "Training a Haar Cascade is time-consuming. To make a decent classifier you would need thousands of positive and negative images and some computing power. To save you some time and effort, you will be using a pre-trained model. Don't worry, you still need to learn how to use the functions with Zumi! </font>\n",
    "\n",
    "## Code\n",
    "\n",
    "### Import libraries\n",
    "\n",
    "<font size=3> The code that you will need for detecting stop signs will be in the vision library. You will also need the camera and zumi libraries!</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zumi.zumi import Zumi\n",
    "from zumi.util.screen import Screen\n",
    "from zumi.util.vision import Vision \n",
    "from zumi.util.camera import Camera \n",
    "import time\n",
    "\n",
    "zumi = Zumi()\n",
    "camera = Camera()\n",
    "screen = Screen()\n",
    "vision = Vision()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### detect_stop_sign()\n",
    "<font size=3> This function is part of the vision library and will return True if a stop sign is detected, and False if not detected. It takes in an image parameter, so you will need to take a picture and pass it into the function. The function turns your image into grayscale for you. Here's an example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.start_camera()  # Turn on the camera\n",
    "img = camera.capture()  # Take and save an image\n",
    "print(vision.detect_stop_sign(img))  #Print True or False\n",
    "camera.show_image(img)  # Show the image in the console\n",
    "camera.close()  # Don't forget to close the camera!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity: Drive and Stop\n",
    "\n",
    "<font size=3> Detecting stop signs and taking the right actions is extremely important for a self-driving car. In this activity, you will write a code so that Zumi drives forward until a stop sign is detected. What will the pseudocode look like?<br><br>\n",
    "    \n",
    "<font face=\"Courier\">\n",
    "take a picture <br>\n",
    "check if a stop sign is detected<br>\n",
    "if true, stop and break loop <br>\n",
    "else, take a forward step<br>\n",
    "repeat 100 times <br>  \n",
    "</font>\n",
    "    \n",
    "We started the code for you below. Try it before you scroll down to see the solution!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.start_camera()\n",
    "for i in range(100):\n",
    "    \n",
    "    # TODO: Take a picture\n",
    "    # TODO Call detect_stop_sign\n",
    "    # TODO If true, break the loop\n",
    "    # TODO Else, call forward_step\n",
    "    \n",
    "\n",
    "zumi.stop()\n",
    "camera.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: Drive and Stop\n",
    "<font size=3> If you were having some trouble with the solution, take a look at two possible solutions below: </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zumi.reset_gyro()\n",
    "camera.start_camera()\n",
    "for i in range(100):\n",
    "    \n",
    "    img = camera.capture()  # Take a picture\n",
    "    stop_sign_detect = vision.detect_stop_sign(img)  # Call detect_stop_sign\n",
    "    \n",
    "    if stop_sign_detect:  # If true, break the loop\n",
    "        break\n",
    "    else:                 # Else, call forward_step\n",
    "        zumi.forward_step(20,0)\n",
    "    \n",
    "\n",
    "zumi.stop()\n",
    "camera.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.start_camera()\n",
    "stop_sign_detect = False # initialize the condition to be False\n",
    "\n",
    "try:\n",
    "    while not stop_sign_detect:\n",
    "    \n",
    "        img = camera.capture()  # Take a picture\n",
    "        stop_sign_detect = vision.detect_stop_sign(img)  # Call detect_stop_sign\n",
    "        zumi.forward_step(20,0)\n",
    "finally:\n",
    "    print(\"Done\")\n",
    "    zumi.stop()\n",
    "    camera.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Wait and Drive\n",
    "<font size=3> In this example, Zumi will wait while a stop sign is detected, then drive forward once a stop sign is not detected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.start_camera()\n",
    "stop_sign_detect = True # initialize the condition to be True\n",
    "\n",
    "try:\n",
    "    while stop_sign_detect:\n",
    "        img = camera.capture()  # Take a picture\n",
    "        stop_sign_detect = vision.detect_stop_sign(img)      \n",
    "        \n",
    "finally:\n",
    "    print(\"done\")\n",
    "    zumi.forward(duration=3)\n",
    "    camera.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
